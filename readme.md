# ğŸ” LLM-based RAG Search System

This project is a **Retrieval-Augmented Generation (RAG)** application powered by **Large Language Models (LLMs)**. It combines real-time web search, article scraping, and LLM-based contextual answer generation into a simple, interactive interface using **Flask (backend)** and **Streamlit (frontend)**.

---

## ğŸš€ Features

- ğŸ” Search the web for recent articles related to a query
- ğŸ§  Uses scraped article content to answer contextually
- ğŸ—£ï¸ Maintains conversation history (memory) using LangChain
- ğŸ”„ Supports **Together API** (Mistral) for LLM responses
- ğŸ–¥ï¸ Simple, clean UI with Streamlit

---

## ğŸ“ Project Structure

llm_search_template/
â”‚
â”œâ”€â”€ flask_app/ # Backend Flask app
â”‚ â”œâ”€â”€ app.py # Flask server
â”‚ â””â”€â”€ utils.py # Core logic: search, scrape, answer generation
â”‚
â”œâ”€â”€ streamlit_app/ # Streamlit frontend
â”‚ â””â”€â”€ app.py # User interface
â”‚
â”œâ”€â”€ .env.example # Example environment config (no secrets)
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # You are here


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/llm_search_template.git
cd llm_search_template
```

### 2ï¸âƒ£ Create Virtual Environment
With venv
``` bash
python -m venv env
source env/bin/activate      # Windows: env\Scripts\activate
```

Or with conda
``` bash
conda create --name llm_rag python=3.8
conda activate llm_rag
```

### 3ï¸âƒ£ Install Requirements
``` bash
pip install -r requirements.txt
```

## ğŸ” Environment Variables
Create a .env file in the root directory with the following:
``` bash
SEARCH_API_KEY=google_cse_api_key
SEARCH_ENGINE_ID=google_search_engine_id
TOGETHER_API_KEY=together_api_key
# Optional for future use:
# OPENAI_API_KEY=openai_api_key
```

## â–¶ï¸ Running the App
Start Backend (Flask)
``` bash
cd flask_app
python app.py
```

Start Frontend (Streamlit)
In a new terminal:
``` bash
cd streamlit_app
streamlit run app.py
```

##ğŸ’¡ How It Works
-User enters a query in Streamlit UI

-Flask API receives it and performs:

-Google Search (CSE API)

-Scraping relevant articles (via BeautifulSoup)

-Compiling context and calling Together API (Mistral model)

-Answer is generated and sent back to frontend

-Optionally uses LangChain Memory to hold session history


## ğŸ“¦ Requirements
-Python 3.8+

-Flask

-Streamlit

-requests, BeautifulSoup

-langchain, langchain-openai, langchain-together

-python-dotenv


## ğŸ›¡ï¸ Disclaimer
-This app does not have real-time awareness â€” it generates responses based on scraped article content and model predictions.

